{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediation Analysis with Partial Correlation\n",
    "\n",
    "### Part 1\n",
    "Various functions for naive mediation analysis with partial correlation, including a weigthed variation.\n",
    "Partial correlation is calculated as follows:\n",
    "\n",
    "$ \\hat{\\rho}_{XY \\dot{Z}} = \\frac{N\\sum_{i=1}^N e_{X,i}e_{Y,i}}{\\sqrt{N\\sum_{i=1}^N e_{X,i}^2}\\sqrt{N\\sum_{i=1}^N e_{Y,i}^2}}$\n",
    "\n",
    "Where $e_{X,i}, e_{Y,i}$ are the residuals.\n",
    "\n",
    "Here samples are analysed with respect to the given weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.special import betainc \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn as sl\n",
    "from sklearn import linear_model, datasets, preprocessing, metrics\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P value calc for pearson test\n",
    "\n",
    "def my_pvalue(r, n):\n",
    "    df = n - 2\n",
    "    t_squared = r**2 * (df / ((1.0 - r) * (1.0 + r)))\n",
    "    x = df / (df + t_squared)\n",
    "    x = np.array(x)\n",
    "    x = np.where(x < 1.0, x, 1.0)\n",
    "\n",
    "    return betainc(0.5*df, 0.5, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from math import sqrt\n",
    "\n",
    "def wpearson(vec_1, vec_2, weights, r = 4):\n",
    "\n",
    "\tlist_length = len(vec_1)\n",
    "\n",
    "\ttry:\n",
    "\t\tweights = list(map(float,weights))\n",
    "\texcept:\n",
    "\t\tprint('Invalid weights.')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tvec_1 = list(map(float,vec_1))\n",
    "\t\tvec_2 = list(map(float,vec_2))\n",
    "\t\tif any(len(x) != list_length for x in [vec_2,weights]):\n",
    "\t\t\tprint('Vector/Weight sizes not equal.')\n",
    "\texcept Exception as e:\n",
    "\t\tprint('Invalid vectors. \\n{}'.format(e.with_traceback))\n",
    "\n",
    "\n",
    "# Find total weight sum.\n",
    "\n",
    "\tw_sum = sum(weights)\n",
    "\n",
    "# Calculate the weighted average relative value of vector 1 and vector 2.\n",
    "\tvec1_sum = 0.0\n",
    "\tvec2_sum = 0.0\n",
    "\n",
    "\tfor x in range(len(vec_1)):\n",
    "\t\tvec1_sum += (weights[x] * vec_1[x])\n",
    "\t\tvec2_sum += (weights[x] * vec_2[x])\n",
    "\tvec1_avg = (vec1_sum / w_sum)\n",
    "\tvec2_avg = (vec2_sum / w_sum)\n",
    "\n",
    "# Calculate wPCC\n",
    "\n",
    "\tsum_top = 0.0\n",
    "\tsum_bottom1 = 0.0\n",
    "\tsum_bottom2 = 0.0\n",
    "\n",
    "\tfor x in range(len(vec_1)):\n",
    "\t\tdif_1 = (vec_1[x] - vec1_avg)\n",
    "\t\tdif_2 = (vec_2[x] - vec2_avg)\n",
    "\t\tsum_top += (weights[x] * dif_1 * dif_2)\n",
    "\t\tsum_bottom1 += (dif_1**2)*(weights[x])\n",
    "\t\tsum_bottom2 += (dif_2**2)*(weights[x])\n",
    "\n",
    "\tcor = sum_top / (sqrt(sum_bottom1 * sum_bottom2))\n",
    "\n",
    "\treturn round(cor,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a weighted calculation t\n",
    "weights = np.load('weights.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, linalg\n",
    "\n",
    "def partial_corr(C, W = None):\n",
    "    \"\"\"\n",
    "    Returns the sample linear partial correlation coefficients between pairs of variables in C, controlling \n",
    "    for the remaining variables in C.\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n, p)\n",
    "        Array with the different variables. Each column of C is taken as a variable\n",
    "    Returns\n",
    "    weight : weights for calculating weighted correlation\n",
    "    -------\n",
    "    P : array-like, shape (p, p)\n",
    "        P[i, j] contains the partial correlation of C[:, i] and C[:, j] controlling\n",
    "        for the remaining variables in C.\n",
    "    \"\"\"\n",
    "\n",
    "    C = np.asarray(C)\n",
    "    \n",
    "    W = np.ones(C.shape[0]) if W is None else W\n",
    "    \n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p), dtype=np.float)\n",
    "    P_value = np.zeros((p, p), dtype=np.float)\n",
    "\n",
    "    for i in range(p):\n",
    "        P_corr[i, i] = 1\n",
    "        P_value[i, i] = 1\n",
    "        for j in range(i+1, p):\n",
    "            idx = np.ones(p, dtype=np.bool)\n",
    "            idx[i] = False\n",
    "            idx[j] = False\n",
    "            \n",
    "            #Calcultae residuals given weights\n",
    "            A_j = C[:, j]\n",
    "            A_i = C[:, i]\n",
    "            B = C[:, idx]\n",
    "            \n",
    "            #Find betas with linear regression\n",
    "            reg_j = LinearRegression(fit_intercept=False)\n",
    "            reg_j.fit(A_j.reshape(-1, 1),B, sample_weight=W )\n",
    "            reg_j.fit(B,A_j)\n",
    "\n",
    "            beta_i = reg_j.coef_\n",
    "            \n",
    "            reg_i = LinearRegression(fit_intercept=False)\n",
    "            reg_i.fit(A_i.reshape(-1,1),B,W )\n",
    "            reg_i.fit(B,A_i)\n",
    "            beta_j = reg_i.coef_\n",
    "  \n",
    "            \n",
    "            beta_i = linalg.lstsq(B, A_j)[0]\n",
    "            beta_j = linalg.lstsq(B, A_i)[0]\n",
    "\n",
    "# #             print(B.dot(beta_i).reshape(1,-1))\n",
    "# #             print(A_i)\n",
    "            res_j = A_j - B.dot(beta_i)\n",
    "            res_i = A_i - B.dot(beta_j)\n",
    "\n",
    "            corr = wpearson(res_i, res_j, W)\n",
    "\n",
    "            P_corr[i, j] = corr\n",
    "            P_corr[j, i] = corr\n",
    "            \n",
    "#             p_value = stats.pearsonr(res_i, res_j)[1]\n",
    "            p_value = my_pvalue(corr, SAMPLE_SIZE)\n",
    "            P_value[i, j] = p_value\n",
    "            P_value[j, i] = p_value\n",
    "\n",
    "    return P_corr,P_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "More = pd.read_csv('More.csv')\n",
    "More = More.astype(float)\n",
    "SAMPLE_SIZE = More.shape[0]\n",
    "\n",
    "# mini test\n",
    "Mini = More[['maud5','eatd_py','any_manic_py','any_anx_py','any_disphor_py','eae_violence_1']]\n",
    "partial_corr_array = Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "\n",
    "# Check len in pxl\n",
    "def get_width(num_chars):\n",
    "    return int((1 + num_chars) *256)\n",
    "\n",
    "# PC mat to preaty\n",
    "def allVSall_to_df(mat, trio):\n",
    "    mat = np.delete(mat, 0, 0)\n",
    "    mat = np.delete(mat, 0, 1)\n",
    "    \n",
    "    df = pd.DataFrame(mat)\n",
    "    df.columns = trio\n",
    "    df['Var'] = trio\n",
    "    df = df.set_index('Var')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.    ,  0.5278,  0.5771,  0.0489],\n",
       "       [ 0.5278,  1.    , -0.0785,  0.0711],\n",
       "       [ 0.5771, -0.0785,  1.    , -0.0276],\n",
       "       [ 0.0489,  0.0711, -0.0276,  1.    ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = partial_corr(partial_corr_array)\n",
    "\n",
    "conditianl_cov_for_all = pair[0]\n",
    "conditianl_pvalue_for_all = pair[1]\n",
    "conditianl_cov_for_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Trio analysis - scaling up inputs. Analysing multiple trio, conditioning on different var at a time as suspected mediator.\n",
    "Output: Trio's Excel which marks significant decrease in correlation after conditioning. \n",
    "**Note: Make sure initial correlation is large enough.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['All_EAE', 'cann_init_age_1', 'maud5'], ['All_EAE', 'nico_init_age_1', 'maud5'], ['All_EAE', 'alc_init_age_1', 'maud5'], ['All_EAE', 'not_alc_nico_cann_init_age_1', 'maud5'], ['All_EAE', 'ppyaud5', 'maud5'], ['All_EAE', 'pnicdep5', 'maud5'], ['All_EAE', 'any_anx_py', 'maud5'], ['All_EAE', 'any_manic_py', 'maud5'], ['All_EAE', 'eatd_py', 'maud5'], ['All_EAE', 'personality_dis2', 'maud5'], ['All_EAE', 'any_disphor_py', 'maud5'], ['All_EAE', 'any_disphor_ppy', 'maud5'], ['All_EAE', 'isel_app_1', 'maud5'], ['All_EAE', 'isel_app_2', 'maud5'], ['All_EAE', 'isel_app_3', 'maud5'], ['All_EAE', 'isel_app_4', 'maud5'], ['All_EAE', 'isel_app_5', 'maud5'], ['All_EAE', 'isel_bel_1', 'maud5'], ['All_EAE', 'isel_bel_2', 'maud5'], ['All_EAE', 'isel_bel_3', 'maud5'], ['All_EAE', 'isel_bel_4', 'maud5'], ['All_EAE', 'isel_bel_5', 'maud5'], ['All_EAE', 'isel_tan_1', 'maud5'], ['All_EAE', 'isel_tan_2', 'maud5'], ['All_EAE', 'isel_tan_3', 'maud5'], ['All_EAE', 'isel_tan_4', 'maud5'], ['All_EAE', 'isel_tan_5', 'maud5']]\n"
     ]
    }
   ],
   "source": [
    "M_s = ['cann_init_age_1', 'nico_init_age_1','alc_init_age_1','not_alc_nico_cann_init_age_1',\n",
    "      'ppyaud5','pnicdep5','any_anx_py','any_manic_py','eatd_py',\n",
    "       'personality_dis2','any_disphor_py','any_disphor_ppy','isel_app_1', 'isel_app_2',\n",
    "       'isel_app_3', 'isel_app_4', 'isel_app_5', 'isel_bel_1', 'isel_bel_2',\n",
    "       'isel_bel_3', 'isel_bel_4', 'isel_bel_5', 'isel_tan_1', 'isel_tan_2',\n",
    "       'isel_tan_3', 'isel_tan_4', 'isel_tan_5']\n",
    "\n",
    "\n",
    "# Customize trios\n",
    "# CheckList1 =[['personality_dis2','nico_init_age_1','maud5'],['alc_init_age_1','pnicdep5','maud5'],\n",
    "#              ['eae_sexual','alc_init_age_1','pyaud5'],['alc_init_age_1','eae_violence_1','pyaud5'],\n",
    "#             ['alc_init_age_1','eae_sexual','any_sud_py'],['alc_init_age_1','eae_violence_1','any_sud_py']]\n",
    "\n",
    "# comibinations\n",
    "CheckList1 = []\n",
    "EAE_event = 'All_EAE' # change accordingly: 'eae_violence_1'/ ''eae_neglect_1' etc.\n",
    "\n",
    "for m in M_s:\n",
    "    tr = [EAE_event, m,'maud5']\n",
    "    CheckList1.append(tr)\n",
    "\n",
    "print(CheckList1)\n",
    "# More.corr()['n14q7_4']['anysud_maud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "from copy import deepcopy\n",
    "\n",
    "# Patterns\n",
    "# headers\n",
    "header_pattern = xlwt.Pattern()\n",
    "header_pattern.pattern = xlwt.Pattern.SOLID_PATTERN\n",
    "header_pattern.pattern_fore_colour = 30\n",
    "header_font = xlwt.Font()\n",
    "header_font.bold = True\n",
    "header_font.colour_index = 9\n",
    "align = xlwt.Alignment()\n",
    "align.horz = xlwt.Alignment.HORZ_CENTER\n",
    "thick_border = xlwt.Borders()\n",
    "thick_border.right = xlwt.Borders.THICK\n",
    "thick_border.left = xlwt.Borders.THICK\n",
    "thick_border.top = xlwt.Borders.THICK\n",
    "thick_border.bottom = xlwt.Borders.THICK\n",
    "\n",
    "header_style = xlwt.XFStyle()\n",
    "header_style.pattern = header_pattern\n",
    "header_style.borders = thick_border\n",
    "header_style.font = header_font\n",
    "\n",
    "# normal cell\n",
    "\n",
    "cell_pattern = xlwt.Pattern()\n",
    "cell_pattern.pattern = xlwt.Pattern.SOLID_PATTERN\n",
    "cell_pattern.pattern_fore_colour = 9\n",
    "cell_font = xlwt.Font()\n",
    "border = xlwt.Borders()\n",
    "border.right = xlwt.Borders.THIN\n",
    "border.left = xlwt.Borders.THIN\n",
    "border.top = xlwt.Borders.THIN\n",
    "border.bottom = xlwt.Borders.THIN\n",
    "\n",
    "cell_style = xlwt.XFStyle()\n",
    "cell_style.pattern = cell_pattern\n",
    "cell_style.borders = border\n",
    "\n",
    "# side\n",
    "\n",
    "side_style = deepcopy(cell_style)\n",
    "side_style.pattern.pattern_fore_colour = 22\n",
    "side_style.alignment.wrap = 1\n",
    "\n",
    "# delta style \n",
    "delta_style = deepcopy(cell_style)\n",
    "delta_style.num_format_str = '0%'\n",
    "\n",
    "# value style\n",
    "value_style = deepcopy(cell_style)\n",
    "value_style.num_format_str = 'General'\n",
    "\n",
    "# noticeable style\n",
    "notice_style = deepcopy(delta_style)\n",
    "notice_style.pattern.pattern_fore_colour = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['All_EAE', 'cann_init_age_1', 'maud5']\n",
      "[[ 1.      0.4568  0.2204 -0.0057]\n",
      " [ 0.4568  1.      0.1277  0.0375]\n",
      " [ 0.2204  0.1277  1.      0.2331]\n",
      " [-0.0057  0.0375  0.2331  1.    ]]\n",
      "2\n",
      "['All_EAE', 'nico_init_age_1', 'maud5']\n",
      "[[1.     0.4666 0.2716 0.0152]\n",
      " [0.4666 1.     0.1356 0.0546]\n",
      " [0.2716 0.1356 1.     0.1028]\n",
      " [0.0152 0.0546 0.1028 1.    ]]\n",
      "3\n",
      "['All_EAE', 'alc_init_age_1', 'maud5']\n",
      "[[ 1.      0.3478  0.7151 -0.0075]\n",
      " [ 0.3478  1.      0.0917  0.0643]\n",
      " [ 0.7151  0.0917  1.      0.0539]\n",
      " [-0.0075  0.0643  0.0539  1.    ]]\n",
      "4\n",
      "['All_EAE', 'not_alc_nico_cann_init_age_1', 'maud5']\n",
      "[[1.     0.3883 0.1195 0.0266]\n",
      " [0.3883 1.     0.1213 0.0498]\n",
      " [0.1195 0.1213 1.     0.1521]\n",
      " [0.0266 0.0498 0.1521 1.    ]]\n",
      "5\n",
      "['All_EAE', 'ppyaud5', 'maud5']\n",
      "[[1.     0.4315 0.1661 0.0264]\n",
      " [0.4315 1.     0.1242 0.0532]\n",
      " [0.1661 0.1242 1.     0.1233]\n",
      " [0.0264 0.0532 0.1233 1.    ]]\n",
      "6\n",
      "['All_EAE', 'pnicdep5', 'maud5']\n",
      "[[1.     0.4177 0.1416 0.029 ]\n",
      " [0.4177 1.     0.1346 0.0526]\n",
      " [0.1416 0.1346 1.     0.1181]\n",
      " [0.029  0.0526 0.1181 1.    ]]\n",
      "7\n",
      "['All_EAE', 'any_anx_py', 'maud5']\n",
      "[[1.     0.375  0.1142 0.0377]\n",
      " [0.375  1.     0.1193 0.0616]\n",
      " [0.1142 0.1193 1.     0.0623]\n",
      " [0.0377 0.0616 0.0623 1.    ]]\n",
      "8\n",
      "['All_EAE', 'any_manic_py', 'maud5']\n",
      "[[1.     0.2253 0.0251 0.0432]\n",
      " [0.2253 1.     0.0669 0.0627]\n",
      " [0.0251 0.0669 1.     0.096 ]\n",
      " [0.0432 0.0627 0.096  1.    ]]\n",
      "9\n",
      "['All_EAE', 'eatd_py', 'maud5']\n",
      "[[1.     0.2137 0.0289 0.0452]\n",
      " [0.2137 1.     0.0373 0.069 ]\n",
      " [0.0289 0.0373 1.     0.0159]\n",
      " [0.0452 0.069  0.0159 1.    ]]\n",
      "10\n",
      "['All_EAE', 'personality_dis2', 'maud5']\n",
      "[[1.     0.3569 0.0729 0.0339]\n",
      " [0.3569 1.     0.1896 0.0387]\n",
      " [0.0729 0.1896 1.     0.152 ]\n",
      " [0.0339 0.0387 0.152  1.    ]]\n",
      "11\n",
      "['All_EAE', 'any_disphor_py', 'maud5']\n",
      "[[1.     0.3702 0.119  0.0326]\n",
      " [0.3702 1.     0.1251 0.0564]\n",
      " [0.119  0.1251 1.     0.0987]\n",
      " [0.0326 0.0564 0.0987 1.    ]]\n",
      "12\n",
      "['All_EAE', 'any_disphor_ppy', 'maud5']\n",
      "[[1.     0.4303 0.1712 0.0342]\n",
      " [0.4303 1.     0.1504 0.0595]\n",
      " [0.1712 0.1504 1.     0.0611]\n",
      " [0.0342 0.0595 0.0611 1.    ]]\n",
      "13\n",
      "['All_EAE', 'isel_app_1', 'maud5']\n",
      "[[1.     0.2048 0.034  0.0452]\n",
      " [0.2048 1.     0.0369 0.0691]\n",
      " [0.034  0.0369 1.     0.0127]\n",
      " [0.0452 0.0691 0.0127 1.    ]]\n",
      "14\n",
      "['All_EAE', 'isel_app_2', 'maud5']\n",
      "[[1.     0.2272 0.0504 0.0435]\n",
      " [0.2272 1.     0.0483 0.0677]\n",
      " [0.0504 0.0483 1.     0.0383]\n",
      " [0.0435 0.0677 0.0383 1.    ]]\n",
      "15\n",
      "['All_EAE', 'isel_app_3', 'maud5']\n",
      "[[1.     0.3548 0.1624 0.0409]\n",
      " [0.3548 1.     0.044  0.0686]\n",
      " [0.1624 0.044  1.     0.0223]\n",
      " [0.0409 0.0686 0.0223 1.    ]]\n",
      "16\n",
      "['All_EAE', 'isel_app_4', 'maud5']\n",
      "[[1.     0.4319 0.2391 0.0441]\n",
      " [0.4319 1.     0.0148 0.0696]\n",
      " [0.2391 0.0148 1.     0.0018]\n",
      " [0.0441 0.0696 0.0018 1.    ]]\n",
      "17\n",
      "['All_EAE', 'isel_app_5', 'maud5']\n",
      "[[ 1.      0.5125  0.5813  0.0539]\n",
      " [ 0.5125  1.     -0.065   0.0677]\n",
      " [ 0.5813 -0.065   1.     -0.0269]\n",
      " [ 0.0539  0.0677 -0.0269  1.    ]]\n",
      "18\n",
      "['All_EAE', 'isel_bel_1', 'maud5']\n",
      "[[1.     0.2177 0.0452 0.0454]\n",
      " [0.2177 1.     0.0359 0.0694]\n",
      " [0.0452 0.0359 1.     0.0051]\n",
      " [0.0454 0.0694 0.0051 1.    ]]\n",
      "19\n",
      "['All_EAE', 'isel_bel_2', 'maud5']\n",
      "[[1.     0.2505 0.073  0.0446]\n",
      " [0.2505 1.     0.0384 0.0691]\n",
      " [0.073  0.0384 1.     0.0114]\n",
      " [0.0446 0.0691 0.0114 1.    ]]\n",
      "20\n",
      "['All_EAE', 'isel_bel_3', 'maud5']\n",
      "[[1.     0.3797 0.1643 0.041 ]\n",
      " [0.3797 1.     0.0668 0.068 ]\n",
      " [0.1643 0.0668 1.     0.0222]\n",
      " [0.041  0.068  0.0222 1.    ]]\n",
      "21\n",
      "['All_EAE', 'isel_bel_4', 'maud5']\n",
      "[[ 1.      0.4536  0.257   0.0477]\n",
      " [ 0.4536  1.      0.008   0.0697]\n",
      " [ 0.257   0.008   1.     -0.0111]\n",
      " [ 0.0477  0.0697 -0.0111  1.    ]]\n",
      "22\n",
      "['All_EAE', 'isel_bel_5', 'maud5']\n",
      "[[ 1.      0.5302  0.5281  0.0443]\n",
      " [ 0.5302  1.     -0.0741  0.0687]\n",
      " [ 0.5281 -0.0741  1.     -0.0097]\n",
      " [ 0.0443  0.0687 -0.0097  1.    ]]\n",
      "23\n",
      "['All_EAE', 'isel_tan_1', 'maud5']\n",
      "[[ 1.      0.1961  0.0282  0.0457]\n",
      " [ 0.1961  1.      0.0301  0.0697]\n",
      " [ 0.0282  0.0301  1.     -0.003 ]\n",
      " [ 0.0457  0.0697 -0.003   1.    ]]\n",
      "24\n",
      "['All_EAE', 'isel_tan_2', 'maud5']\n",
      "[[1.     0.2221 0.0552 0.0438]\n",
      " [0.2221 1.     0.0354 0.0685]\n",
      " [0.0552 0.0354 1.     0.0304]\n",
      " [0.0438 0.0685 0.0304 1.    ]]\n",
      "25\n",
      "['All_EAE', 'isel_tan_3', 'maud5']\n",
      "[[1.     0.3569 0.1732 0.0409]\n",
      " [0.3569 1.     0.042  0.0687]\n",
      " [0.1732 0.042  1.     0.0212]\n",
      " [0.0409 0.0687 0.0212 1.    ]]\n",
      "26\n",
      "['All_EAE', 'isel_tan_4', 'maud5']\n",
      "[[1.     0.4306 0.22   0.0405]\n",
      " [0.4306 1.     0.0425 0.0688]\n",
      " [0.22   0.0425 1.     0.0172]\n",
      " [0.0405 0.0688 0.0172 1.    ]]\n",
      "27\n",
      "['All_EAE', 'isel_tan_5', 'maud5']\n",
      "[[ 1.      0.5186  0.5854  0.0589]\n",
      " [ 0.5186  1.     -0.0776  0.0667]\n",
      " [ 0.5854 -0.0776  1.     -0.0347]\n",
      " [ 0.0589  0.0667 -0.0347  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "#Check trios\n",
    "from itertools import combinations\n",
    "\n",
    "# Create new workbook object\n",
    "book = xlwt.Workbook(encoding='utf-8')\n",
    "sheet1 = book.add_sheet('eae')\n",
    "\n",
    "max_pair_width = 0\n",
    "max_npc_width = 0\n",
    "min_head_width = 0\n",
    "\n",
    "Corr = More.corr()\n",
    "\n",
    "header_list = ['Obeserved Vars', 'Pairs', 'Baseline Corr', 'P_value (2t)', 'Naïve Partial Corr', 'Naïve Partial Corr', 'Delta', 'P_value (2t)']\n",
    "row_index = 0\n",
    "i = 0\n",
    "for trio in CheckList1:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    print(trio)\n",
    "\n",
    "    for idx, head in enumerate(header_list):\n",
    "        sheet1.write(row_index, idx, head, header_style)\n",
    "        \n",
    "        current_len = get_width(len(head))\n",
    "        if current_len > min_head_width:\n",
    "            min_head_width = current_len\n",
    "    row_index += 1\n",
    "    \n",
    "    trio_label = ','.join(trio)\n",
    "    sheet1.write_merge(row_index, row_index + 2, 0, 0, trio_label, style = side_style)\n",
    "    \n",
    "    #The checked sub-df\n",
    "    Trios = More[trio]\n",
    "    Trios = np.hstack((np.ones((Trios.shape[0],1)), Trios))\n",
    "    partial_corr_array = Trios\n",
    "    pair = partial_corr(partial_corr_array, weights)\n",
    "\n",
    "    conditianl_cov_for_all = pair[0]\n",
    "    conditianl_pvalue_for_all = pair[1]\n",
    "    print(conditianl_cov_for_all)\n",
    "    \n",
    "    corrdf = allVSall_to_df(conditianl_cov_for_all,trio)\n",
    "    pvaluedf = allVSall_to_df(conditianl_pvalue_for_all,trio)\n",
    "    \n",
    "    \n",
    "    for idx, pair in enumerate(combinations(trio, 2)):\n",
    "        pair_label = ','.join(pair)\n",
    "        sheet1.write(row_index + idx, 1, pair_label, cell_style)\n",
    "        \n",
    "        current_len = get_width(len(pair_label))\n",
    "        if current_len > max_pair_width:\n",
    "            max_pair_width = current_len\n",
    "        \n",
    "#         baseline_corr = Corr.loc[pair[0], pair[1]]\n",
    "#         baseline_pvalue = stats.pearsonr(More[pair[0]], More[pair[1]])[1]\n",
    "        \n",
    "        baseline_corr = wpearson(More[pair[0]],More[pair[1]],weights)\n",
    "        baseline_pvalue = my_pvalue(baseline_corr, More.shape[0])\n",
    "    \n",
    "        sheet1.write(row_index + idx, 2, baseline_corr, value_style)\n",
    "        sheet1.write(row_index + idx, 3, baseline_pvalue, value_style)\n",
    "        \n",
    "        npc_label = \"{},{}|{}\".format(*pair, *set(trio).difference(pair))\n",
    "        npc = corrdf.loc[pair[0], pair[1]]\n",
    "        pvalue = pvaluedf.loc[pair[0], pair[1]]\n",
    "        delta = (baseline_corr - npc) / baseline_corr\n",
    "        \n",
    "        current_len = get_width(len(npc_label))\n",
    "        if current_len > max_npc_width:\n",
    "            max_npc_width = current_len\n",
    "        \n",
    "        sheet1.write(row_index + idx, 4, npc_label, cell_style)\n",
    "        sheet1.write(row_index + idx, 5, npc, value_style)\n",
    "        \n",
    "        if delta > 0.2:\n",
    "            sheet1.write(row_index + idx, 6, delta, notice_style)\n",
    "        else:\n",
    "            sheet1.write(row_index + idx, 6, delta, delta_style)\n",
    "            \n",
    "        sheet1.write(row_index + idx, 7, pvalue, value_style)\n",
    "        \n",
    "    row_index += 3\n",
    "\n",
    "for i in range(7):\n",
    "    sheet1.col(i).width = min_head_width\n",
    "sheet1.col(1).width = max_pair_width\n",
    "sheet1.col(4).width = max_npc_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.save(f\"{EAE_event}_maud.xls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Extra exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.    ,  0.1292,  0.1441,  0.0057,  0.0094,  0.0652],\n",
       "        [ 0.1292,  1.    ,  0.1911, -0.0134,  0.1206,  0.1298],\n",
       "        [ 0.1441,  0.1911,  1.    ,  0.0079,  0.0185,  0.2349],\n",
       "        [ 0.0057, -0.0134,  0.0079,  1.    , -0.0231,  0.0453],\n",
       "        [ 0.0094,  0.1206,  0.0185, -0.0231,  1.    ,  0.0012],\n",
       "        [ 0.0652,  0.1298,  0.2349,  0.0453,  0.0012,  1.    ]]),\n",
       " {'any_sud_py': 0,\n",
       "  'cann_init_age_1': 1,\n",
       "  'personality_dis2': 2,\n",
       "  'nethrace_2': 3,\n",
       "  'nsex_1': 4,\n",
       "  'eae_violence_1': 5})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def weighted_corr_mat(colnames, weights = weights):\n",
    "    ind = {}\n",
    "    for index, value in enumerate(colnames):\n",
    "        ind[value] = index\n",
    "    \n",
    "    pairs = itertools.combinations(colnames, 2)\n",
    "    pairs = list(pairs)\n",
    "    weight_corr = np.eye(len(colnames))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        a = pair[0]\n",
    "        b = pair[1]\n",
    "        \n",
    "        corr = wpearson(More[a], More[b], weights)\n",
    "        weight_corr[ind[a]][ind[b]] = corr\n",
    "        weight_corr[ind[b]][ind[a]] = corr\n",
    "\n",
    "    return(weight_corr, ind)\n",
    "#     return allVSall_to_df(weight_corr, trio=colnames)\n",
    "    \n",
    "weighted_corr_mat(['any_sud_py','cann_init_age_1','personality_dis2','nethrace_2','nsex_1','eae_violence_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltacorr(var, row_name, col_name, color='#ffffcc'):\n",
    "\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    delta = abs(var - weighted_corr_mat[row_name][col_name]) / abs(weighted_corr_mat[row_name][col_name])\n",
    "    if delta > 0.2: return color \n",
    "    else: return ''\n",
    "        \n",
    "# deltacorr(corrdf['any_sud_py'])\n",
    "# type(corrdf['any_sud_py'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maud5</th>\n",
       "      <th>cann_init_age_1</th>\n",
       "      <th>personality_dis2</th>\n",
       "      <th>nethrace_2</th>\n",
       "      <th>nsex_1</th>\n",
       "      <th>eae_violence_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>maud5</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cann_init_age_1</th>\n",
       "      <td>0.2091</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>-0.0264</td>\n",
       "      <td>0.1102</td>\n",
       "      <td>0.0841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personality_dis2</th>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0051</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>0.2118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nethrace_2</th>\n",
       "      <td>0.0482</td>\n",
       "      <td>-0.0264</td>\n",
       "      <td>-0.0051</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0225</td>\n",
       "      <td>0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nsex_1</th>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.1102</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>-0.0225</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eae_violence_1</th>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.2118</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>-0.0136</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   maud5  cann_init_age_1  personality_dis2  nethrace_2  \\\n",
       "Var                                                                       \n",
       "maud5             1.0000           0.2091            0.1148      0.0482   \n",
       "cann_init_age_1   0.2091           1.0000            0.1369     -0.0264   \n",
       "personality_dis2  0.1148           0.1369            1.0000     -0.0051   \n",
       "nethrace_2        0.0482          -0.0264           -0.0051      1.0000   \n",
       "nsex_1            0.0328           0.1102           -0.0052     -0.0225   \n",
       "eae_violence_1    0.0221           0.0841            0.2118      0.0448   \n",
       "\n",
       "                  nsex_1  eae_violence_1  \n",
       "Var                                       \n",
       "maud5             0.0328          0.0221  \n",
       "cann_init_age_1   0.1102          0.0841  \n",
       "personality_dis2 -0.0052          0.2118  \n",
       "nethrace_2       -0.0225          0.0448  \n",
       "nsex_1            1.0000         -0.0136  \n",
       "eae_violence_1   -0.0136          1.0000  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take from DB (wave3_NS4) only VariablesForUse columns\n",
    "trio = ['maud5','cann_init_age_1',\n",
    "       'personality_dis2','nethrace_2','nsex_1','eae_violence_1']\n",
    "# 'pnicdep5','any_anx_py','any_manic_py','eatd_py',\n",
    "#  'ppyaud5','nico_init_age_1','alc_init_age_1',,'any_disphor_py','any_disphor_ppy'\n",
    "Trios = More[trio]\n",
    "Trios = np.hstack((np.ones((Trios.shape[0],1)), Trios))\n",
    "partial_corr_array = Trios\n",
    "pair = partial_corr(partial_corr_array, weights)\n",
    "\n",
    "conditianl_cov_for_all = pair[0]\n",
    "conditianl_pvalue_for_all = pair[1]\n",
    "\n",
    "corrdf = allVSall_to_df(conditianl_cov_for_all,trio)\n",
    "pvaluedf = allVSall_to_df(conditianl_pvalue_for_all,trio)\n",
    "# weighted_corr = weighted_corr_mat(trio)\n",
    "\n",
    "\n",
    "corrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     25122\n",
       "0     7486\n",
       "1     3701\n",
       "Name: cann_py, dtype: int64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB['cann_py'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
